{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:42:56.881901Z",
     "start_time": "2020-08-02T09:42:46.739708Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from utility.utility import ndcg, mAP_score, highlight, history_plot, generate_pairwise_dataset\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 9999)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:57:36.466034Z",
     "start_time": "2020-08-02T09:57:36.420060Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv_stack(tf.keras.layers.Layer):\n",
    "    def __init__(self, lg, nf):\n",
    "        super(Conv_stack, self).__init__(name='ConV_stack')\n",
    "        self.lg = lg\n",
    "        self.nf = nf\n",
    "        self.conv_dict = {}\n",
    "        for i in range(2, self.lg+1):\n",
    "            self.conv_dict[i] = tf.keras.layers.Conv2D(self.nf, i, strides=(1, 1), padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)\n",
    "        x_1 = inputs\n",
    "        x = {}\n",
    "        for i in range(2, self.lg+1):\n",
    "            x[i] = self.conv_dict[i](inputs)\n",
    "        return tf.keras.layers.concatenate([x_1] + [x[k] for k in x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:57:36.746874Z",
     "start_time": "2020-08-02T09:57:36.732882Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dim_wise_max_pooling(tf.keras.layers.Layer):\n",
    "    def __init__(self, lg, nf):\n",
    "        super(Dim_wise_max_pooling, self).__init__(name='dim_wise_max_pooling')\n",
    "        self.lg = lg\n",
    "        self.nf = nf\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        channel_range = [self.nf*i+1 for i in range(self.lg)]\n",
    "        x_1 = inputs[:, :, :, 0]\n",
    "        x = {}\n",
    "        for i in range(2, self.lg+1):\n",
    "            x[i] = tf.reduce_max(inputs[:, :, :, channel_range[i-2]:channel_range[i-1]], axis=-1)\n",
    "            \n",
    "        return tf.keras.layers.concatenate([x_1] + [x[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:57:38.354955Z",
     "start_time": "2020-08-02T09:57:38.306980Z"
    }
   },
   "outputs": [],
   "source": [
    "class Row_wise_max_pooling(tf.keras.layers.Layer):\n",
    "    def __init__(self, ns, lg, firstk):\n",
    "        super(Row_wise_max_pooling, self).__init__(name='row_wise_max_pooling')\n",
    "        self.ns = ns\n",
    "        self.lg = lg\n",
    "        self.firstk = firstk\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = {}\n",
    "        for i in range(1, self.lg+1):\n",
    "            x[i] = tf.math.top_k(inputs[:, :, self.firstk*(i-1):self.firstk*i], k=self.ns)[0]\n",
    "            \n",
    "        return tf.keras.layers.concatenate([x[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:57:38.945614Z",
     "start_time": "2020-08-02T09:57:38.911635Z"
    }
   },
   "outputs": [],
   "source": [
    "class Idf_concat(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Idf_concat, self).__init__(name='idf_concat')\n",
    "        \n",
    "    def call(self, inputs, idf):\n",
    "        expand_idf = tf.expand_dims(idf, axis=-1)\n",
    "        return tf.keras.layers.concatenate([inputs, expand_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:57:38.977597Z",
     "start_time": "2020-08-02T09:57:38.957608Z"
    }
   },
   "outputs": [],
   "source": [
    "class PACCR(tf.keras.Model):\n",
    "    def __init__(self, firstk, lq, lg, nf, ns):\n",
    "        super(PACCR, self).__init__(name='')\n",
    "        self.firstk = firstk\n",
    "        self.lq = lq\n",
    "        self.lg = lg\n",
    "        self.nf = nf\n",
    "        self.ns = ns\n",
    "        \n",
    "        self.conv_stack = Conv_stack(lg=self.lg, nf=self.nf)\n",
    "        self.dim_wise_max_pooling = Dim_wise_max_pooling(lg=self.lg, nf=self.nf)\n",
    "        self.row_wise_max_pooling = Row_wise_max_pooling(lg=self.lg, ns=self.ns, firstk=self.firstk)\n",
    "        self.idf_concat = Idf_concat()\n",
    "\n",
    "    def call(self, inputs, idf):\n",
    "        x = self.conv_stack(inputs)\n",
    "        x = self.dim_wise_max_pooling(x)\n",
    "        x = self.row_wise_max_pooling(x)\n",
    "        x = self.idf_concat(x, idf)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:59:17.865996Z",
     "start_time": "2020-08-02T09:59:17.850003Z"
    }
   },
   "outputs": [],
   "source": [
    "class DRMM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DRMM, self).__init__(name='DRMM')\n",
    "        initializer = tf.keras.initializers.he_normal()\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation='relu', kernel_initializer=initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='relu', kernel_initializer=initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='relu', kernel_initializer=initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = tf.squeeze(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:59:18.955373Z",
     "start_time": "2020-08-02T09:59:18.942380Z"
    }
   },
   "outputs": [],
   "source": [
    "class PACCR_DRMM(tf.keras.Model):\n",
    "    def __init__(self, firstk, lq, lg, nf, ns):\n",
    "        super(PACCR_DRMM, self).__init__(name='')\n",
    "        self.firstk = firstk\n",
    "        self.lq = lq\n",
    "        self.lg = lg\n",
    "        self.nf = nf\n",
    "        self.ns = ns\n",
    "        \n",
    "        self.paccr = PACCR(firstk=self.firstk, lq=self.lq, lg=self.lg, nf=self.nf, ns=self.ns)\n",
    "        self.drmm = DRMM()\n",
    "        \n",
    "    def call(self, inputs, idf):\n",
    "        x = self.paccr(inputs, idf)\n",
    "        x = self.drmm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:59:19.676959Z",
     "start_time": "2020-08-02T09:59:19.663967Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pairwise_PACCR_DRMM(tf.keras.Model):\n",
    "    def __init__(self, firstk, lq, lg, nf, ns):\n",
    "        super(Pairwise_PACCR_DRMM, self).__init__(name='Pairwise_PACCR_DRMM')\n",
    "        self.Paccr_Drmm = PACCR_DRMM(firstk, lq, lg, nf, ns)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        positive_sim = inputs['positive_sim_matrix']\n",
    "        negative_sim = inputs['negative_sim_matrix']\n",
    "        idf_softmax = inputs['idf_softmax']\n",
    "        \n",
    "        positive = self.Paccr_Drmm(positive_sim, idf_softmax)\n",
    "        negative = self.Paccr_Drmm(negative_sim, idf_softmax)\n",
    "        \n",
    "        return tf.concat([positive, negative], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bert\n",
    "# test = pd.read_csv('./data/paccr_drmm_bert_test_all.csv', converters={\"query_idf\": literal_eval,\n",
    "#                                                                       \"idf_softmax\": literal_eval,\n",
    "#                                                                       \"sim_matrix\": literal_eval,\n",
    "#                                                                       \"query_token\": literal_eval,\n",
    "#                                                                       \"product_title_token\": literal_eval,\n",
    "#                                                                       \"token_ids\": literal_eval,\n",
    "#                                                                       \"drmm_hist\": literal_eval,\n",
    "#                                                                       'token': literal_eval})\n",
    "\n",
    "# test['binary_relevance'] = test['median_relevance'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "# df = generate_pairwise_dataset(test)\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# dev_q = set(random.sample(list(df['query'].unique()), 45))\n",
    "# train_q = set(df['query'].unique()) - dev_q\n",
    "\n",
    "# train = pd.concat([df.groupby('query').get_group(name) for name in train_q]).sample(frac=1).reset_index(drop=True)\n",
    "# dev = pd.concat([df.groupby('query').get_group(name) for name in dev_q]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# metadata = {'negative_sim_matrix': tf.constant(train['sim_matrix_N'].tolist(), dtype=tf.float32), \n",
    "#             'positive_sim_matrix': tf.constant(train['sim_matrix_P'].tolist(), dtype=tf.float32),\n",
    "#             'idf_softmax': tf.constant(train['idf_softmax'].tolist(), dtype=tf.float32)}\n",
    "\n",
    "# metadata_dev ={'negative_sim_matrix': tf.constant(dev['sim_matrix_N'].tolist(), dtype=tf.float32), \n",
    "#             'positive_sim_matrix': tf.constant(dev['sim_matrix_P'].tolist(), dtype=tf.float32),\n",
    "#             'idf_softmax': tf.constant(dev['idf_softmax'].tolist(), dtype=tf.float32)}\n",
    "\n",
    "# firstk = 13\n",
    "# lq = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "df = pd.read_csv('./data/paccr_drmm_.csv', converters={\"negative_sim_matrix\": literal_eval,\n",
    "                                                      \"positive_sim_matrix\": literal_eval,\n",
    "                                                      \"idf_softmax\": literal_eval})\n",
    "df = df[['query_preprocessed', 'negative_sim_matrix', 'positive_sim_matrix', 'idf_softmax']]\n",
    "\n",
    "test = pd.read_csv('./data/paccr_drmm_test.csv', converters={\"hist\": literal_eval,\n",
    "                                                             \"query_idf\": literal_eval,\n",
    "                                                             \"sim_matrix\": literal_eval,\n",
    "                                                             \"idf_softmax\": literal_eval})\n",
    "\n",
    "dev_q = set(random.sample(list(df['query_preprocessed'].unique()), 45))\n",
    "train_q = set(df['query_preprocessed'].unique()) - dev_q\n",
    "\n",
    "train = pd.concat([df.groupby('query_preprocessed').get_group(name) for name in train_q]).sample(frac=1).reset_index(drop=True)\n",
    "dev = pd.concat([df.groupby('query_preprocessed').get_group(name) for name in dev_q]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "metadata = {'negative_sim_matrix': tf.constant(train['negative_sim_matrix'].tolist(), dtype=tf.float32), \n",
    "            'positive_sim_matrix': tf.constant(train['positive_sim_matrix'].tolist(), dtype=tf.float32),\n",
    "            'idf_softmax': tf.constant(train['idf_softmax'].tolist(), dtype=tf.float32)}\n",
    "\n",
    "metadata_dev ={'negative_sim_matrix': tf.constant(dev['negative_sim_matrix'].tolist(), dtype=tf.float32), \n",
    "            'positive_sim_matrix': tf.constant(dev['positive_sim_matrix'].tolist(), dtype=tf.float32),\n",
    "            'idf_softmax': tf.constant(dev['idf_softmax'].tolist(), dtype=tf.float32)}\n",
    "\n",
    "firstk = 8\n",
    "lq = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(metadata)\n",
    "ds = ds.shuffle(buffer_size=len(train))\n",
    "batchs = 128\n",
    "ds = ds.batch(batchs).repeat()\n",
    "example_batch = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:55:58.291100Z",
     "start_time": "2020-08-02T10:55:58.080221Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = Pairwise_PACCR_DRMM(firstk, lq, lg, nf, ns)\n",
    "n = 20\n",
    "lg = 5\n",
    "nf = 32\n",
    "ns = 2\n",
    "learning_rate = 1\n",
    "print_step = 10\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "\n",
    "Pairwise_ranking_loss(y_true=None, y_pred=model(example_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:57:13.396457Z",
     "start_time": "2020-08-02T10:55:59.016686Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_sum = 0\n",
    "ndcg_sum = 0\n",
    "step_history = []\n",
    "loss_history = []\n",
    "loss_history_dev = []\n",
    "ndcg_history = []\n",
    "\n",
    "start = time.time()\n",
    "for step, batch_train in enumerate(ds):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_train)\n",
    "        loss_value = Pairwise_ranking_loss(y_true=None, y_pred=logits)\n",
    "        loss_sum += loss_value \n",
    "        \n",
    "        if step == 0:\n",
    "            loss_history_dev.append(Pairwise_ranking_loss(y_true=None, y_pred=model(dev_set)))\n",
    "            \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    if step % print_step == 0:\n",
    "        current_loss_average = float(loss_sum)/print_step\n",
    "        if step ==0:\n",
    "            current_loss_average = loss_sum\n",
    "\n",
    "        logits_dev = model(metadata_dev)\n",
    "        current_loss_average_dev = Pairwise_ranking_loss(y_true=None, y_pred=logits_dev)\n",
    "            \n",
    "        for q in dev_q:\n",
    "            ndcg_test = test[test['query_preprocessed'] == q]\n",
    "            metadata_ndcg = {'sim_matrix': tf.constant(ndcg_test['sim_matrix'].tolist(), dtype=tf.float32), \n",
    "                             'idf_softmax': tf.constant(ndcg_test['idf_softmax'].tolist(), dtype=tf.float32)}\n",
    "            \n",
    "            ndcg_test['rel'] = model.predict(metadata_ndcg).numpy()\n",
    "            rel_pred = list(ndcg_test.sort_values(by=['rel'], axis=0, ascending=False)['median_relevance']-1)\n",
    "            ndcg_sum += ndcg(rel_pred, p=n, form=\"exp\")\n",
    "            \n",
    "        current_ndcg_average = ndcg_sum/len(dev_q)\n",
    "        step_history.append(step)\n",
    "        loss_history.append(current_loss_average)\n",
    "        loss_history_dev.append(current_loss_average_dev)\n",
    "        ndcg_history.append(current_ndcg_average)\n",
    "        \n",
    "        print(\"Training loss at step %d: %.5f, dev_loss : %.5f, nDCG@20 : %.5f\"% (step, \n",
    "                                                                  current_loss_average, \n",
    "                                                                  current_loss_average_dev,\n",
    "                                                                  current_ndcg_average))\n",
    "        print(\"Seen so far: %s train samples, learning rate: %.4f\" % ((step + 1) * batchs, learning_rate))\n",
    "        ndcg_sum = 0\n",
    "        loss_sum = 0\n",
    "        start = time.time()\n",
    "        \n",
    "    if step == 200:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
