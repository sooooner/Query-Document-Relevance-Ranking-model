{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T10:22:37.272321Z",
     "start_time": "2020-08-01T10:22:30.116422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "import sentencepiece as spm\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.raw_ops import RaggedTensorToVariant\n",
    "\n",
    "@tf.RegisterGradient(\"RaggedTensorFromVariant\")\n",
    "def _RaggedTensorFromVariantGrad(*args):\n",
    "    if len(args) == 2:\n",
    "        op, grad = args\n",
    "        res = [RaggedTensorToVariant(rt_nested_splits=[], rt_dense_values=grad,\n",
    "                                      batched_input=False)]\n",
    "    else:\n",
    "        op, empty, grad = args\n",
    "        res = [RaggedTensorToVariant(rt_nested_splits=[op.outputs[0]], rt_dense_values=grad,\n",
    "                                    batched_input=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T08:42:31.378846Z",
     "start_time": "2020-08-01T08:42:31.137986Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv').fillna('')\n",
    "# test = pd.read_csv('./data/test.csv').fillna('')\n",
    "# test_df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['query'] = train.apply(lambda x: x['query'].lower(), axis=1)\n",
    "train['product_title'] = train.apply(lambda x: x['product_title'].lower(), axis=1)\n",
    "train['product_description'] = train.apply(lambda x: x['product_description'].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = list(train['query'].unique()) + \\\n",
    "                  list(train['product_title'].unique()) + \\\n",
    "                  list(train['product_description'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./sentences_.txt', 'w', encoding='utf8') as f:\n",
    "#     f.write('\\n'.join(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = '--input={} --model_prefix={} --vocab_size={} --user_defined_symbols={} --model_type={}'\n",
    "\n",
    "input_file = './sentences_.txt'\n",
    "model_prefix = 'sentences_'\n",
    "vocab_size = 2000\n",
    "user_defined_symbols = '▁[PAD],▁[UNK],▁[CLS],▁[SEP],▁[MASK]'\n",
    "model_type = 'bpe'\n",
    "\n",
    "cmd = parameter.format(input_file, model_prefix, vocab_size, user_defined_symbols, model_type)\n",
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = pd.read_csv('./sentences_.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_size = len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = './sentences_.model'\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['query_processed'] = train.apply(lambda x: sp.encode_as_ids(x['query']), axis=1)\n",
    "train['product_title_processed'] = train.apply(lambda x: sp.encode_as_ids(x['product_title']), axis=1)\n",
    "train['product_description_processed'] = train.apply(lambda x: sp.encode_as_ids(x['product_description']), axis=1)\n",
    "train['NSP_label'] = train.apply(lambda x: 0 if random.uniform(0, 1) <0.5 else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dict = {}\n",
    "for q in train['query'].unique():\n",
    "    q_dict[q] = train.groupby('query').get_group(q)['product_title_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(train['query'].unique())\n",
    "def sentence_change(x):\n",
    "    if x['NSP_label'] == 0:\n",
    "        tmp = random.choice(temp)\n",
    "        while tmp == x['query']:\n",
    "            tmp = random.choice(temp)\n",
    "        return random.choice(list(q_dict[tmp]))\n",
    "    return x['product_title_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['changed_product_title'] = train.apply(lambda x: sentence_change(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask(x):\n",
    "    new_query = []\n",
    "    masked_indexes = []\n",
    "    for i, w in enumerate(x):\n",
    "        if random.uniform(0, 1) <= 0.15:\n",
    "            if random.uniform(0, 1) <= 0.8:\n",
    "                new_query.append(7)\n",
    "                masked_indexes.append(i)\n",
    "            elif random.uniform(0, 1) <= 0.5:\n",
    "                random_word = random.randint(8, vocab_size-8)\n",
    "                new_query.append(random_word)\n",
    "                masked_indexes.append(i)\n",
    "            else :\n",
    "                new_query.append(w)\n",
    "                masked_indexes.append(i)\n",
    "        else :\n",
    "            new_query.append(w)\n",
    "    return new_query, masked_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x):\n",
    "    query, label = _mask(x['query_processed'])\n",
    "    x['masked_query'] = [5] + query + [6]\n",
    "    query_label = [l+1 for l in label]\n",
    "    title, label = _mask(x['changed_product_title'])\n",
    "    x['masked_product_title'] = title + [6]\n",
    "    title_label = [l+len(x['masked_query']) for l in label]\n",
    "    x['LM_label_idx'] = query_label + title_label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(test_func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair_label(x):\n",
    "    return [5] + x['query_processed'] + [6] + x['changed_product_title'] + [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['original_label'] = train.apply(lambda x: gen_pair_label(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LM_label(x):\n",
    "    return [x['original_label'][i] for i in x['LM_label_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LM_label'] = train.apply(lambda x: get_LM_label(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 217, 66, 31, 1803, 627, 905, 6, 103, 7, 598, 1953, 7, 1952, 7, 1235, 6]\n",
      "[5, 217, 66, 31, 1803, 627, 905, 6, 103, 268, 598, 1953, 831, 1952, 1304, 1235, 6]\n",
      "[CLS] bridal shower decorations [SEP] v [MASK] otw [MASK]b [MASK] backpack [SEP]\n",
      "[CLS] bridal shower decorations [SEP] vans otw washburn backpack [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train['masked_query'].iloc[0] + train['masked_product_title'].iloc[0])\n",
    "print(train['original_label'][0])\n",
    "\n",
    "print(sp.DecodeIds(train['masked_query'].iloc[0] + train['masked_product_title'].iloc[0]))\n",
    "print(sp.DecodeIds(train['original_label'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# position embedding max_len\n",
    "max_len = train.masked_query.map(len).max() + train.masked_product_title.map(len).max()\n",
    "train, dev = train_test_split(train, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voca 수\n",
    "vocab_size = len(vocab_list)\n",
    "\n",
    "# 임베딩 벡터의 크기\n",
    "d_model = 256\n",
    "\n",
    "# encoder layer 수\n",
    "num_layers = 4\n",
    "\n",
    "# attentin 수\n",
    "num_heads = 4\n",
    "depth = d_model/num_heads\n",
    "\n",
    "# max_len = train.masked_query.map(len).max() + train.masked_product_title.map(len).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {'masked_query' : tf.ragged.constant(train['masked_query'], dtype=tf.int32, ragged_rank=1),\n",
    "            'masked_product_title' : tf.ragged.constant(train['masked_product_title'], dtype=tf.int32, ragged_rank=1),\n",
    "            'LM_label_idx' : tf.ragged.constant(train['LM_label_idx'], ragged_rank=1),\n",
    "            'NSP_label' : tf.constant(train['NSP_label']),\n",
    "            'LM_label' : tf.ragged.constant(train['LM_label'], dtype=tf.int32, ragged_rank=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dev = {'masked_query' : tf.ragged.constant(dev['masked_query'], dtype=tf.int32, ragged_rank=1),\n",
    "            'masked_product_title' : tf.ragged.constant(dev['masked_product_title'], dtype=tf.int32, ragged_rank=1),\n",
    "            'LM_label_idx' : tf.ragged.constant(dev['LM_label_idx'], ragged_rank=1),\n",
    "            'NSP_label' : tf.constant(dev['NSP_label']),\n",
    "            'LM_label' : tf.ragged.constant(dev['LM_label'], dtype=tf.int32, ragged_rank=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dev = tf.data.Dataset.from_tensor_slices(metadata_dev)\n",
    "ds_dev = ds_dev.shuffle(buffer_size=len(dev))\n",
    "batchs_dev = 32\n",
    "ds_devs = ds_dev.batch(batchs_dev)\n",
    "example_batch_dev = next(iter(ds_devs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(metadata)\n",
    "\n",
    "ds = ds.shuffle(buffer_size=len(train))\n",
    "batchs = 128\n",
    "ds = ds.batch(batchs).repeat()\n",
    "example_batch = next(iter(ds))\n",
    "# example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len, name='Bert_Embedding'):\n",
    "        super(Bert_Embedding, self).__init__(name=name)\n",
    "        self._supports_ragged_inputs = True \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.Token_Embedding = tf.Variable(tf.random.truncated_normal([self.vocab_size, self.d_model],\n",
    "                                                                      stddev=1.0 / np.sqrt(self.d_model)), \n",
    "                                           trainable=True)\n",
    "        self.Segment_Embedding = tf.Variable(tf.random.truncated_normal([2, self.d_model],\n",
    "                                                                        stddev=1.0 / np.sqrt(self.d_model)), \n",
    "                                             trainable=True)\n",
    "        self.Position_Embedding = tf.Variable(tf.random.truncated_normal([self.max_len, self.d_model],\n",
    "                                                                         stddev=1.0 / np.sqrt(self.d_model)), \n",
    "                                              trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sentence_pair = tf.concat([inputs['masked_query'], inputs['masked_product_title']], axis=1)\n",
    "        T_embedding = tf.nn.embedding_lookup(self.Token_Embedding, sentence_pair)\n",
    "        \n",
    "        Sa = tf.zeros_like(inputs['masked_query'])\n",
    "        Sb = tf.ones_like(inputs['masked_product_title'])\n",
    "        S_embedding = tf.gather(self.Segment_Embedding, tf.concat([Sa, Sb], axis=1))\n",
    "        \n",
    "        elems = tf.math.reduce_sum(tf.ones_like(sentence_pair), axis=1)\n",
    "        ragged_range = tf.map_fn(tf.range, elems, fn_output_signature=tf.RaggedTensorSpec(shape=[None], dtype=tf.int32))\n",
    "#         P_embedding = tf.ragged.map_flat_values(tf.nn.embedding_lookup, self.Position_Embedding, ragged_range)\n",
    "        P_embedding = tf.gather(self.Position_Embedding, ragged_range)\n",
    "    \n",
    "        return tf.math.add_n([T_embedding, S_embedding, P_embedding])\n",
    "        \n",
    "    def compute_output_shape(self, inputs):\n",
    "        return (inputs['masked_query'].shape[0], None, self.d_model)\n",
    "    \n",
    "# bert_embedding = Bert_Embedding(vocab_size, d_model, max_len)\n",
    "# result = bert_embedding(example_batch)\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaled_Dot_Product_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name='Scaled_Dot_Product_Attention'):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__(name=name)\n",
    "        self._supports_ragged_inputs = True\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = self.d_model/self.num_heads\n",
    "        self.Q_layer = tf.keras.layers.Dense(units=self.depth)\n",
    "        self.K_layer = tf.keras.layers.Dense(units=self.depth)\n",
    "        self.V_layer = tf.keras.layers.Dense(units=self.depth)\n",
    "    \n",
    "    def QKV_Gen(self, inputs):\n",
    "        Query = tf.ragged.map_flat_values(self.Q_layer, inputs)\n",
    "        Key = tf.ragged.map_flat_values(self.K_layer, inputs)\n",
    "        Value = tf.ragged.map_flat_values(self.V_layer, inputs)\n",
    "        return Query, Key, Value\n",
    "\n",
    "#     def calculate_attention(self, x):\n",
    "#         matmul_qk = tf.matmul(x[0], x[1], transpose_b=True)\n",
    "#         logits = matmul_qk / tf.math.sqrt(depth)\n",
    "#         attention_weights = tf.nn.softmax(logits)\n",
    "#         return tf.matmul(attention_weights, x[2])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         query, key, value = self.QKV_Gen(inputs)\n",
    "#         attention_value=tf.map_fn(fn=self.calculate_attention, \n",
    "#                                   elems=(query, key, value),\n",
    "#                                   dtype=tf.float32,\n",
    "#                                   fn_output_signature=tf.RaggedTensorSpec(shape=[None, int(self.depth)], ragged_rank=0))\n",
    "#         return attention_value\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value = self.QKV_Gen(inputs)\n",
    "        matmul_qk = tf.ragged.map_flat_values(tf.matmul, query, key, transpose_b=True) \n",
    "        logits = matmul_qk / tf.math.sqrt(self.depth)\n",
    "        attention_weights = tf.ragged.map_flat_values(tf.nn.softmax, logits, axis=-1) \n",
    "        attention_value = tf.ragged.map_flat_values(tf.matmul, attention_weights, value) \n",
    "        return attention_value    \n",
    "    \n",
    "    def compute_output_shape(self, inputs):\n",
    "        return (inputs.shape[0], None, self.d_model)\n",
    "    \n",
    "# scaled_Dot_Product_Attention = Scaled_Dot_Product_Attention(d_model, num_heads, name='Scaled_Dot_Product_Attention')\n",
    "# qattention_value = scaled_Dot_Product_Attention(result)\n",
    "# attention_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self._supports_ragged_inputs = True\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.scaled_dot_product_attention_dict = {}\n",
    "        for i in range(self.num_heads):\n",
    "            self.scaled_dot_product_attention_dict[i] = Scaled_Dot_Product_Attention(self.d_model, self.num_heads)\n",
    "        self.drop_out = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.norm =  tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.W = tf.Variable(initial_value=w_init(shape=(input_shape[-1], self.d_model), dtype='float32'), trainable=True)\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(self.d_model,), dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_value_dict = {}\n",
    "        for i in range(self.num_heads):\n",
    "            attention_value_dict[i] = self.scaled_dot_product_attention_dict[i](inputs)\n",
    "        concat_attention = tf.concat([attention_value_dict[i] for i in range(self.num_heads)], axis=-1)\n",
    "        \n",
    "        outputs = self.drop_out(tf.ragged.map_flat_values(tf.matmul, concat_attention, self.W) + self.b)\n",
    "        return tf.ragged.map_flat_values(self.norm, tf.math.add(outputs, inputs))\n",
    "    \n",
    "    def compute_output_shape(self, inputs):\n",
    "        return (inputs.shape[0], None, self.d_model)r\n",
    "    \n",
    "    \n",
    "# multiheadattention = MultiHeadAttention(d_model, num_heads)\n",
    "# multiheadattentionmatrix = multiheadattention(result)\n",
    "# # multiheadattentionmatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_and_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='Add_and_Norm'):\n",
    "        super(Add_and_Norm, self).__init__(name=name)\n",
    "        self._supports_ragged_inputs = True\n",
    "        self.drop_out = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.norm =  tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, residual):\n",
    "        outputs = self.drop_out(inputs)\n",
    "        outputs = tf.ragged.map_flat_values(self.norm, tf.math.add(outputs, residual)) \n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, inputs):\n",
    "        return (inputs.shape)\n",
    "    \n",
    "# add_and_norm = Add_and_Norm()\n",
    "# norm = add_and_norm(multiheadattentionmatrix, result)\n",
    "# # norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_NN(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, name='Feed_Forward_NN'):\n",
    "        super(Feed_Forward_NN, self).__init__(name=name)\n",
    "        self._supports_ragged_inputs = True\n",
    "        self.d_model = d_model\n",
    "        self.dff = self.d_model*4\n",
    "        self.layer1 = tf.keras.layers.Dense(units=self.dff, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.drop_out = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.norm =  tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.ragged.map_flat_values(self.layer1, inputs) \n",
    "        x = tf.ragged.map_flat_values(self.layer2, x) \n",
    "        x = self.drop_out(x)\n",
    "        return tf.ragged.map_flat_values(self.norm, tf.math.add(x, inputs)) \n",
    "    \n",
    "# ffnn = FFNN(d_model)\n",
    "# output = ffnn(multiheadattentionmatrix, result)\n",
    "# # output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, max_len, num_layers, name='BERT'):\n",
    "        super(BERT, self).__init__(name=name)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.bert_embedding = Bert_Embedding(self.vocab_size, self.d_model, self.max_len)\n",
    "        self.ffnn_dict = {}\n",
    "        self.mha_dict = {}\n",
    "        for i in range(self.num_layers):\n",
    "            self.mha_dict[i] = MultiHeadAttention(self.d_model, self.num_heads, name='MultiHeadAttention_%d'%(i+1))\n",
    "            self.ffnn_dict[i] = Feed_Forward_NN(self.d_model, name='Feed_Forward_NN_%d'%(i+1))\n",
    "        self.lm_layer = tf.keras.layers.Dense(self.vocab_size, activation='softmax', name='lm_layer')\n",
    "        self.nsp_layer = tf.keras.layers.Dense(2, activation='softmax', name='nsp_layer')\n",
    "        self.finetune_layer = tf.keras.layers.Dense(4, activation='softmax', name='finetune_layer')\n",
    "        \n",
    "    def call(self, inputs, mode='pretrain'):\n",
    "        x = self.bert_embedding(inputs)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.mha_dict[i](x)\n",
    "            x = self.ffnn_dict[i](x)\n",
    "        lm_x = tf.map_fn(fn=lambda rt: tf.gather(rt[0], rt[1]), \n",
    "                         elems=(x, inputs['LM_label_idx']),\n",
    "                         dtype=tf.float32,\n",
    "                         fn_output_signature=tf.RaggedTensorSpec(ragged_rank=0))\n",
    "        lm = tf.ragged.map_flat_values(self.lm_layer, lm_x) \n",
    "        nsp = self.nsp_layer(x[:,:1].to_tensor())\n",
    "        pred = self.finetune_layer(x[:,:1].to_tensor())\n",
    "\n",
    "        if mode == 'pretrain':\n",
    "            self.finetune_layer.trainable = False\n",
    "            self.lm_layer.trainable = True\n",
    "            self.nsp_layer.trainable = True\n",
    "            return lm, tf.squeeze(nsp)\n",
    "        elif mode == 'finetune':\n",
    "            self.lm_layer.trainable = False\n",
    "            self.nsp_layer.trainable = False\n",
    "            self.finetune_layer.trainable = True\n",
    "            return tf.squeeze(pred)\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        self.lm_layer.trainable = False\n",
    "        self.nsp_layer.trainable = False\n",
    "        self.finetune_layer.trainable = False\n",
    "        return self.call(inputs, mode='finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pairwise_loss(y_true, y_pred):\n",
    "    lm_true = y_true['LM_label']   \n",
    "    lm_pred = y_pred[0]\n",
    "    lm_loss = tf.ragged.map_flat_values(tf.keras.losses.sparse_categorical_crossentropy, lm_true, lm_pred)\n",
    "    lm_loss = tf.keras.backend.mean(lm_loss)\n",
    "    \n",
    "    nsp_true = tf.reshape(y_true['NSP_label'], shape=(-1, 1))\n",
    "    nsp_pred = y_pred[1]\n",
    "    nsp_loss = tf.keras.losses.BinaryCrossentropy()(nsp_true, nsp_pred)\n",
    "    return lm_loss + nsp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.625193>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = BERT(vocab_size, d_model, num_heads, max_len, num_layers, name='pretrain')\n",
    "\n",
    "Pairwise_loss(y_true=example_batch, y_pred=model(example_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pretrain\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Bert_Embedding (Bert_Embeddi multiple                  540672    \n",
      "_________________________________________________________________\n",
      "Feed_Forward_NN_1 (Feed_Forw multiple                  526080    \n",
      "_________________________________________________________________\n",
      "Feed_Forward_NN_2 (Feed_Forw multiple                  526080    \n",
      "_________________________________________________________________\n",
      "Feed_Forward_NN_3 (Feed_Forw multiple                  526080    \n",
      "_________________________________________________________________\n",
      "Feed_Forward_NN_4 (Feed_Forw multiple                  526080    \n",
      "_________________________________________________________________\n",
      "MultiHeadAttention_1 (MultiH multiple                  263680    \n",
      "_________________________________________________________________\n",
      "MultiHeadAttention_2 (MultiH multiple                  263680    \n",
      "_________________________________________________________________\n",
      "MultiHeadAttention_3 (MultiH multiple                  263680    \n",
      "_________________________________________________________________\n",
      "MultiHeadAttention_4 (MultiH multiple                  263680    \n",
      "_________________________________________________________________\n",
      "lm_layer (Dense)             multiple                  514000    \n",
      "_________________________________________________________________\n",
      "nsp_layer (Dense)            multiple                  514       \n",
      "_________________________________________________________________\n",
      "finetune_layer (Dense)       multiple                  1028      \n",
      "=================================================================\n",
      "Total params: 4,215,254\n",
      "Trainable params: 4,214,226\n",
      "Non-trainable params: 1,028\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .00001\n",
    "print_step = 10\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0: 8.60710, dev loss : 9.00872, time :  18.680289268493652\n",
      "Training loss at step 10: 8.41563, dev loss : 8.99298, time :  90.47512412071228\n",
      "Training loss at step 20: 8.39720, dev loss : 8.94570, time :  89.21184635162354\n",
      "Training loss at step 30: 8.36994, dev loss : 8.93658, time :  89.20885038375854\n",
      "Training loss at step 40: 8.36278, dev loss : 8.91974, time :  89.76152849197388\n",
      "Training loss at step 50: 8.35014, dev loss : 8.90440, time :  83.96385431289673\n",
      "Training loss at step 60: 8.33517, dev loss : 8.89219, time :  90.35027575492859\n",
      "Training loss at step 70: 8.32502, dev loss : 8.88064, time :  84.92830109596252\n",
      "Training loss at step 80: 8.30131, dev loss : 8.86932, time :  81.28838992118835\n",
      "Training loss at step 90: 8.29911, dev loss : 8.85437, time :  91.16472721099854\n",
      "Training loss at step 100: 8.27529, dev loss : 8.84190, time :  90.00339365005493\n"
     ]
    }
   ],
   "source": [
    "loss_sum = 0\n",
    "loss_history = []\n",
    "loss_history_dev = []\n",
    "\n",
    "start = time.time()\n",
    "for step, batch_train in enumerate(ds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_train, mode='pretrain')\n",
    "        loss_value = Pairwise_loss(y_true=batch_train, y_pred=logits)\n",
    "        loss_sum += loss_value \n",
    "        \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    if step % print_step == 0:\n",
    "        current_loss_average = float(loss_sum)/print_step\n",
    "        if step ==0:\n",
    "            current_loss_average = loss_sum\n",
    "        loss_history.append(current_loss_average)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        loss_sum_dev = 0\n",
    "        for dev_step, batch_dev in enumerate(ds_devs):\n",
    "            logits_dev = model(batch_dev, mode='pretrain')\n",
    "            loss_value_dev = Pairwise_loss(y_true=batch_dev, y_pred=logits_dev)\n",
    "            loss_sum_dev += loss_value_dev \n",
    "        current_loss_average_dev = loss_sum_dev/dev_step\n",
    "        loss_history_dev.append(current_loss_average_dev)\n",
    "        ds_devs = ds_dev.batch(batchs_dev)\n",
    "        print(\"Training loss at step %d: %.5f, dev loss : %.5f, time : \"% (step, current_loss_average, current_loss_average_dev), time.time() - start)\n",
    "\n",
    "#         print(\"Training loss at step %d: %.5f\"% (step, current_loss_average))\n",
    "#         print(\"Seen so far: %s train samples, learning rate: %.5f\" % ((step + 1) * batchs, learning_rate))\n",
    "#         print(\"time :\", time.time() - start)\n",
    "        start = time.time() \n",
    "        \n",
    "    if step == 100:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 312,
   "position": {
    "height": "40px",
    "left": "726px",
    "right": "20px",
    "top": "10px",
    "width": "597px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
